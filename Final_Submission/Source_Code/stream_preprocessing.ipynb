{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tweepy\n",
    "import config\n",
    "import pyspark\n",
    "import emoji\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_unixtime, date_format, col, avg, sum, to_timestamp\n",
    "from pyspark.sql.functions import concat, lit\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.types import StringType\n",
    "import findspark\n",
    "findspark.init()\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v = CountVectorizer(stop_words='english')\n",
    "import csv\n",
    "import timeit\n",
    "import random\n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1: get cryptocurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[['2023-04-24', 0.3839159905910492], ['2023-04-25', 0.3949190080165863], ['2023-04-26', 0.4014979898929596], ['2023-04-27', 0.40984299778938293], ['2023-04-28', 0.4049319922924042], ['2023-04-29', 0.40278398990631104], ['2023-04-30', 0.39735400676727295]]\n"
     ]
    }
   ],
   "source": [
    "# Fetch BTC trade raw data\n",
    "today = date.today()\n",
    "BTC_raw = yf.download('DOGE-USD', start=today-timedelta(days=7), end=today) # BTC, ETH, XRP, ADA, DOGE\n",
    "# BTC_raw.columns = ['date', 'open_price', 'high', 'low', 'close_price', 'adj close', 'volume']\n",
    "\n",
    "# reset index and move Date column to the first column\n",
    "BTC_raw = BTC_raw.reset_index().rename(columns={'Date': 'date'})\n",
    "BTC_raw = BTC_raw.loc[:, ['date', 'Close']]\n",
    "BTC_raw.head()\n",
    "\n",
    "l = len(BTC_raw[\"date\"])\n",
    "a = []\n",
    "for n in range(l):\n",
    "    data = [str(BTC_raw[\"date\"][n])[0:10], float(BTC_raw[\"Close\"][n])]\n",
    "    a.append(data)\n",
    "    \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      date|        close price|\n",
      "+----------+-------------------+\n",
      "|2023-04-24| 0.3839159905910492|\n",
      "|2023-04-25| 0.3949190080165863|\n",
      "|2023-04-26| 0.4014979898929596|\n",
      "|2023-04-27|0.40984299778938293|\n",
      "|2023-04-28| 0.4049319922924042|\n",
      "|2023-04-29|0.40278398990631104|\n",
      "|2023-04-30|0.39735400676727295|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "day_price_data = spark.createDataFrame(a,[\"date\", \"close price\"])\n",
    "\n",
    "day_price_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2: get twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching twitter data...\n",
      "total twitter data: 7209\n",
      "+-------------------+--------------------+--------+-------------------+\n",
      "|               date|                text|follower|                 id|\n",
      "+-------------------+--------------------+--------+-------------------+\n",
      "|2023-04-29 19:57:01|RT @TopCoinLinks:...|     338|1652462019734392834|\n",
      "|2023-04-29 19:55:07|RT @cryptozup: Wh...|    1750|1652461542011535362|\n",
      "|2023-04-29 19:53:46|RT @daddymonkey_e...|       0|1652461201174011906|\n",
      "|2023-04-29 19:52:34|RT @SatoshiOwl: ?...|     150|1652460898957643776|\n",
      "|2023-04-29 19:50:59|RT @daddymonkey_e...|       0|1652460501731876864|\n",
      "|2023-04-29 19:50:32|RT @daddymonkey_e...|       1|1652460388200460289|\n",
      "|2023-04-29 19:47:49|RT @daddymonkey_e...|       0|1652459703383867392|\n",
      "|2023-04-29 19:44:28|1: Bitcoin price ...|      64|1652458860857241600|\n",
      "|2023-04-29 19:38:39|1: Bitcoin price ...|     964|1652457397716230145|\n",
      "|2023-04-29 19:27:53|ðŸ’µ US$ 0.00000000...|    3337|1652454688183164930|\n",
      "|2023-04-29 19:13:36|RT @rovercrc: Do ...|     123|1652451093266288646|\n",
      "|2023-04-29 19:09:27|RT @BitcoinCEOdog...|      84|1652450049337741312|\n",
      "|2023-04-29 19:05:02|Latest #crypto pr...|      19|1652448937855336449|\n",
      "|2023-04-29 19:01:21|RT @TopCoinLinks:...|      48|1652448010146664449|\n",
      "|2023-04-29 19:00:34|Bitcoin price ale...|      32|1652447813303963648|\n",
      "|2023-04-29 19:00:31|Live Crypto Price...|     573|1652447802729938950|\n",
      "|2023-04-29 18:51:28|RT @TopCoinLinks:...|   79306|1652445522462998528|\n",
      "|2023-04-29 18:47:04|$DOGE $USDT #DOGE...|    1684|1652444415187734530|\n",
      "|2023-04-29 18:43:30|1: Bitcoin price ...|      64|1652443517761847304|\n",
      "|2023-04-29 18:42:33|RT @PepogeToken: ...|      55|1652443279697526784|\n",
      "+-------------------+--------------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up Twitter API credentials\n",
    "consumer_key = config.API_KEY\n",
    "consumer_secret= config.API_SECRET\n",
    "access_token= config.ACCESS_TOKEN\n",
    "access_token_secret = config.ACCESS_TOKEN_SECRET\n",
    "\n",
    "# Authenticate with Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "# Query Twitter API for tweets\n",
    "data = tweepy.Cursor(api.search_tweets, q=\"Dogecoin OR DOGE\", until=\"2023-04-30 00:00:00\", lang=\"en\", count=100).items(8000)\n",
    "\n",
    "# Create an empty list to store the processed data\n",
    "processed_data_list = []\n",
    "\n",
    "print(\"fetching twitter data...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        tweet = data.next()\n",
    "            \n",
    "        user_date = tweet.created_at\n",
    "        user_follower = int(tweet.user.followers_count)\n",
    "        user_id = int(tweet.id)\n",
    "        user_text = tweet.text\n",
    "        final_data = [user_date, user_text, user_follower, user_id]   \n",
    "            \n",
    "        processed_data_list.append(final_data)\n",
    "      \n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "print(\"total twitter data:\", len(processed_data_list))\n",
    "\n",
    "# Create a DataFrame from the processed data list\n",
    "twitter_data = spark.createDataFrame(processed_data_list, [\"date\", \"text\", \"follower\", \"id\"])\n",
    "\n",
    "twitter_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis model \n",
    "\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "def get_vader_score(sentence): \n",
    "    compound=analyzer.polarity_scores(sentence)['compound']\n",
    "    if compound > 0.05: \n",
    "        return ['positive',compound]\n",
    "    elif (compound >= -0.05) and (compound <=0.05): \n",
    "        return ['neutral',compound]\n",
    "    else: \n",
    "        return ['negative',compound]\n",
    "\n",
    "def vader_sentiment(X):\n",
    "  vader_result=get_vader_score(X)[0]#X.apply(lambda x: get_vader_score(x)[0])\n",
    "  vader_score = get_vader_score(X)[1]#X.apply(lambda x: get_vader_score(x)[1])\n",
    "  #vader_score_total = np.mean(vader_score)\n",
    "  return vader_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start PySpark transform #####################################################################\n",
    "\n",
    "# load shedding\n",
    "cleaned_twitter_data = twitter_data.filter(col(\"follower\") >= 10)\n",
    "\n",
    "# operator reordering, first use filter bc its selectivity is the highest\n",
    "\n",
    "cleaned_twitter_data = cleaned_twitter_data.filter(col(\"id\") % 10 != random.randint(0, 9))\n",
    "\n",
    "# remove emoji\n",
    "def remove_emoji(col):\n",
    "  result = emoji.replace_emoji(col, replace=\"\")\n",
    "  return result\n",
    "\n",
    "clean_udf = F.UserDefinedFunction(remove_emoji, T.StringType())\n",
    "tweets_df_cleaned = twitter_data.withColumn(\"text\", clean_udf(\"text\"))\n",
    "\n",
    "\n",
    "# remove mention \n",
    "cleaned_twitter_data = twitter_data.withColumn(\"text\", regexp_replace('text', \"@\\s*[A-Za-z0-9_]+\", ''))\n",
    "\n",
    "cleaned_twitter_data = cleaned_twitter_data.withColumn(\"text\", regexp_replace(\"text\", \"#\\s*[A-Za-z0-9_]+\", \"\"))\n",
    "\n",
    "# remove retweet\n",
    "cleaned_twitter_data = cleaned_twitter_data.withColumn(\"text\", regexp_replace('text', \"RT : \", ''))\n",
    "\n",
    "# remove links\n",
    "cleaned_twitter_data = cleaned_twitter_data.withColumn('text', regexp_replace('text', r\"http\\S+\", ''))\n",
    "\n",
    "cleaned_twitter_data = cleaned_twitter_data.withColumn('text', regexp_replace('text', r\"www.\\S+\", ''))\n",
    "\n",
    "# remove next line\n",
    "cleaned_twitter_data = cleaned_twitter_data.withColumn(\"text\", regexp_replace(\"text\", r\"\\n\", \"\"))\n",
    "\n",
    "cleaned_twitter_data = cleaned_twitter_data.withColumn('text', regexp_replace('text', '\\s+', ' '))\n",
    "\n",
    "cleaned_twitter_data = cleaned_twitter_data.select(col('date'), col('text'))\n",
    "\n",
    "# put sentiment analysis here\n",
    "\n",
    "# sentiment model \n",
    "\n",
    "vader_sentiment_udf = F.UserDefinedFunction(vader_sentiment, T.FloatType())\n",
    "vader_sentiment_twitter_data = cleaned_twitter_data.withColumn(\"text\", vader_sentiment_udf(\"text\"))\n",
    "\n",
    "# calculate average\n",
    "\n",
    "vader_sentiment_twitter_data = vader_sentiment_twitter_data.groupBy(date_format(col(\"date\"), \"yyyy-MM-dd\").alias(\"date\"))\n",
    "vader_sentiment_twitter_data = vader_sentiment_twitter_data.agg(avg(\"text\").alias(\"avg_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date|           avg_value|\n",
      "+----------+--------------------+\n",
      "|2023-04-29| 0.15902398995958367|\n",
      "|2023-04-28| 0.03677221499046253|\n",
      "|2023-04-27|0.002421561549821...|\n",
      "|2023-04-26| 0.14959816590738975|\n",
      "|2023-04-25| 0.25151037540430227|\n",
      "|2023-04-24| 0.30741864358273785|\n",
      "|2023-04-23|0.052318421157066065|\n",
      "|2023-04-22| 0.12403524156917407|\n",
      "|2023-04-21|  0.4070333242416382|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_sentiment_twitter_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv\n",
    "final = vader_sentiment_twitter_data.collect()\n",
    "with open(\"./DOGE_vader.csv\", 'w', newline='') as f:\n",
    "    # create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for row in final:\n",
    "        \n",
    "        # write a row to the csv file\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 3: integrate two data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment_twitter_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = vader_sentiment_twitter_data.join(day_price_data,\"date\",\"left\")\n",
    "\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "merged_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
